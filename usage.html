<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="chrome=1" http-equiv="X-UA-Compatible"/>
  <title>
   Galaxy Docker Image by bgruening
  </title>
  <link href="css/landing_page.css" rel="stylesheet"/>
 </head>
 <body>
  <div class="wrapper">
   <header>
    <h1>
     Galaxy Docker Image
    </h1>
    <p>
     Docker Images tracking the stable Galaxy releases
    </p>
    <p class="bold" string="Table of content">
    </p>
    <ul>
     <li>
      <a href="index.html">
       Global description
      </a>
     </li>
     <li>
      <a href="usage.html">
       Usage
      </a>
     </li>
     <li>
      <a href="auto-exec-tools-in-docker.html">
       Enable Galaxy to use BioContainers (Docker)
      </a>
     </li>
     <li>
      <a href="magic-environment-variables.html">
       Magic Environment variables
      </a>
     </li>
     <li>
      <a href="https-support.html">
       HTTPS Support
      </a>
     </li>
     <li>
      <a href="lite-mode.html">
       Lite Mode
      </a>
     </li>
     <li>
      <a href="extending-the-docker-image.html">
       Extending the Docker Image
      </a>
     </li>
     <li>
      <a href="integrating-non-tool-shed-tools-into-the-container.html">
       Integrating non-Tool Shed tools into the container
      </a>
     </li>
     <li>
      <a href="users-passwords.html">
       Users &amp; Passwords
      </a>
     </li>
     <li>
      <a href="development.html">
       Development
      </a>
     </li>
     <li>
      <a href="requirements.html">
       Requirements
      </a>
     </li>
     <li>
      <a href="support-bug-reports.html">
       Support &amp; Bug Reports
      </a>
     </li>
    </ul>
    <p class="view">
     <a href="https://github.com/bgruening/docker-galaxy">
      View the Project on GitHub
      <small>
       bgruening/docker-galaxy
      </small>
     </a>
    </p>
    <ul class="box">
     <li class="box">
      <a href="https://github.com/bgruening/docker-galaxy/zipball/master">
       Download
       <strong>
        ZIP File
       </strong>
      </a>
     </li>
     <li class="box">
      <a href="https://github.com/bgruening/docker-galaxy/tarball/master">
       Download
       <strong>
        TAR Ball
       </strong>
      </a>
     </li>
     <li class="box">
      <a href="https://github.com/bgruening/docker-galaxy">
       View On
       <strong>
        GitHub
       </strong>
      </a>
     </li>
    </ul>
   </header>
   <section>
    <h1>
     Usage
    </h1>
    <p>
     This chapter explains how to launch the container manually.
    </p>
    <p>
     At first you need to install docker. Please follow the
     <a href="https://docs.docker.com/installation/">
      very good instructions
     </a>
     from the Docker project.
    </p>
    <p>
     After the successful installation, all you need to do is:
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 8021:21 -p 8022:22 quay.io/bgruening/galaxy
</code></pre>
    <p>
     I will shortly explain the meaning of all the parameters. For a more detailed description please consult the
     <a href="http://docs.docker.io/">
      docker manual
     </a>
     , it's really worth reading.
    </p>
    <p>
     Let's start:
    </p>
    <ul>
     <li>
      <p>
       <code>
        docker run
       </code>
       will run the Image/Container for you.
      </p>
      <p>
       In case you do not have the Container stored locally, docker will download it for you.
      </p>
     </li>
     <li>
      <p>
       <code>
        -p 8080:80
       </code>
       will make the port 80 (inside of the container) available on port 8080 on your host. Same holds for port 8021 and 8022, that can be used to transfer data via the FTP or SFTP protocol, respectively.
      </p>
      <p>
       Inside the container a nginx Webserver is running on port 80 and that port can be bound to a local port on your host computer. With this parameter you can access your Galaxy
instance via
       <code>
        http://localhost:8080
       </code>
       immediately after executing the command above. If you work with the
       <a href="https://www.docker.com/products/docker-toolbox">
        Docker Toolbox
       </a>
       on Mac or Windows,
you need to connect to the machine generated by 'Docker Quickstart'. You get its IP address from
       <code>
        docker-machine ls
       </code>
       or from the first line in the terminal, e.g.:
       <code>
        docker is configured to use the default machine with IP 192.168.99.100
       </code>
       .
      </p>
     </li>
     <li>
      <p>
       <code>
        quay.io/bgruening/galaxy
       </code>
       is the Image/Container name, that directs docker to the correct path in the
       <a href="https://quay.io/repository/bgruening/galaxy?tab=tags">
        docker index
       </a>
       .
      </p>
     </li>
     <li>
      <p>
       <code>
        -d
       </code>
       will start the docker container in daemon mode.
      </p>
     </li>
    </ul>
    <p>
     For an interactive session, you can execute:
    </p>
    <pre><code class="language-sh">docker run -i -t -p 8080:80 \
    quay.io/bgruening/galaxy \
    /bin/bash
</code></pre>
    <p>
     and run the
     <code>
      startup
     </code>
     script by yourself, to start PostgreSQL, nginx and Galaxy.
    </p>
    <p>
     Docker images are "read-only", all your changes inside one session will be lost after restart. This mode is useful to present Galaxy to your colleagues or to run workshops with it. To install Tool Shed repositories or to save your data you need to export the calculated data to the host computer.
    </p>
    <p>
     Fortunately, this is as easy as:
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 \
    -v /home/user/galaxy_storage/:/export/ \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     With the additional
     <code>
      -v /home/user/galaxy_storage/:/export/
     </code>
     parameter, Docker will mount the local folder
     <code>
      /home/user/galaxy_storage
     </code>
     into the Container under
     <code>
      /export/
     </code>
     . A
     <code>
      startup.sh
     </code>
     script, that is usually starting nginx, PostgreSQL and Galaxy, will recognize the export directory with one of the following outcomes:
    </p>
    <ul>
     <li>
      In case of an empty
      <code>
       /export/
      </code>
      directory, it will move the
      <a href="http://www.postgresql.org/">
       PostgreSQL
      </a>
      database, the Galaxy database directory, Shed Tools and Tool Dependencies and various config scripts to /export/ and symlink back to the original location.
     </li>
     <li>
      In case of a non-empty
      <code>
       /export/
      </code>
      , for example if you continue a previous session within the same folder, nothing will be moved, but the symlinks will be created.
     </li>
    </ul>
    <p>
     This enables you to have different export folders for different sessions - means real separation of your different projects.
    </p>
    <p>
     You can also collect and store
     <code>
      /export/
     </code>
     data of Galaxy instances in a dedicated docker
     <a href="https://docs.docker.com/engine/userguide/dockervolumes/">
      Data  volume Container
     </a>
     created by:
    </p>
    <pre><code class="language-sh">docker create -v /export \
    --name galaxy-store \
    quay.io/bgruening/galaxy \
    /bin/true
</code></pre>
    <p>
     To mount this data volume in a Galaxy container, use the
     <code>
      --volumes-from
     </code>
     parameter:
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 \
    --volumes-from galaxy-store \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     This also allows for data separation, but keeps everything encapsulated within the docker engine (e.g. on OS X within your
     <code>
      $HOME/.docker
     </code>
     folder - easy to backup, archive and restore. This approach, albeit at the expense of disk space, avoids the problems with permissions
     <a href="https://github.com/bgruening/docker-galaxy-stable/issues/68">
      reported
     </a>
     for data export on non-Linux hosts.
    </p>
    <h2>
     Upgrading images
     <a name="Upgrading-images">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     We will release a new version of this image concurrent with every new Galaxy release. For upgrading an image to a new version we have assembled a few hints for you. Please, take in account that upgrading may vary depending on your Galaxy installation, and the changes in new versions. Use this example carefully!
    </p>
    <ul>
     <li>
      Create a test instance with only the database and configuration files. This will allow testing to ensure that things run but won't require copying all of the data.
     </li>
     <li>
      New unmodified configuration files are always stored in a hidden directory called
      <code>
       .distribution_config
      </code>
      . Use this folder to diff your configurations with the new configuration files shipped with Galaxy. This prevents needing to go through the change log files to find out which new files were added or which new features you can activate.
     </li>
    </ul>
    <p>
     Here are 2 suggested upgrade methods, a quick one, and a safer one.
    </p>
    <h3>
     The quick upgrade method
    </h3>
    <p>
     This method involves less data copying, which makes the process quicker, but makes it impossible to downgrade in case of problems.
    </p>
    <p>
     If you are upgrading from &lt;19.05 to &gt;=19.05, you need to migrate the PostgreSQL database, have a look at
     <a href="#Postgresql-migration">
      PostgreSQL migration
     </a>
     .
    </p>
    <ol>
     <li>
      Stop the old Galaxy container
     </li>
    </ol>
    <pre><code class="language-sh">docker stop &lt;old_container_name&gt;
docker pull quay.io/bgruening/galaxy
</code></pre>
    <ol start="2">
     <li>
      Run the container with the updated image
     </li>
    </ol>
    <pre><code class="language-sh">docker run -p 8080:80 -v /data/galaxy-data:/export --name &lt;new_container_name&gt; quay.io/bgruening/galaxy
</code></pre>
    <ol start="3">
     <li>
      Use diff to find changes in the config files (only if you changed any config file).
     </li>
    </ol>
    <pre><code class="language-sh">cd /data/galaxy-data/.distribution_config
for f in *; do echo $f; diff $f ../galaxy/config/$f; read; done
</code></pre>
    <ol start="4">
     <li>
      Upgrade the database schema
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;new_container_name&gt; bash
galaxyctl stop
sh manage_db.sh upgrade
exit
</code></pre>
    <ol start="5">
     <li>
      Restart Galaxy
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;new_container_name&gt; galaxyctl start
</code></pre>
    <p>
     (Alternatively, restart the whole container)
    </p>
    <h3>
     The safe upgrade method
    </h3>
    <p>
     With this method, you keep a backup in case you decide to downgrade, but requires some potentially long data copying.
    </p>
    <ul>
     <li>
      Note that copying database and datasets can be expensive if you have many GB of data.
     </li>
     <li>
      If you are upgrading from &lt;19.05 to &gt;=19.05, you need to migrate the PostgreSQL database, have a look at
      <a href="#Postgresql-migration">
       PostgreSQL migration
      </a>
      .
     </li>
    </ul>
    <ol>
     <li>
      Download newer version of the Galaxy image
     </li>
    </ol>
    <pre><code>$ sudo docker pull quay.io/bgruening/galaxy
</code></pre>
    <ol start="2">
     <li>
      Stop and rename the current galaxy container
     </li>
    </ol>
    <pre><code>$ sudo docker stop galaxy-instance
$ sudo docker rename galaxy-instance galaxy-instance-old
</code></pre>
    <ol start="3">
     <li>
      Rename the data directory (the one that is mounted to /export in the docker)
     </li>
    </ol>
    <pre><code>$ sudo mv /data/galaxy-data /data/galaxy-data-old
</code></pre>
    <ol start="4">
     <li>
      Run a new Galaxy container using newer image and wait while Galaxy generates the default content for /export
     </li>
    </ol>
    <pre><code>$ sudo docker run -p 8080:80 -v /data/galaxy-data:/export --name galaxy-instance quay.io/bgruening/galaxy
</code></pre>
    <ol start="5">
     <li>
      Stop the Galaxy container
     </li>
    </ol>
    <pre><code>$ sudo docker stop galaxy-instance
</code></pre>
    <ol start="6">
     <li>
      Replace the content of the postgres database by the old db data
     </li>
    </ol>
    <pre><code>$ sudo rm -r /data/galaxy-data/postgresql/
$ sudo rsync -var /data/galaxy-data-old/postgresql/  /data/galaxy-data/postgresql/
</code></pre>
    <ol start="7">
     <li>
      Use diff to find changes in the config files (only if you changed any config file).
     </li>
    </ol>
    <pre><code>$ cd /data/galaxy-data/.distribution_config
$ for f in *; do echo $f; diff $f ../../galaxy-data-old/galaxy/config/$f; read; done
</code></pre>
    <ol start="8">
     <li>
      Copy all the users' datasets to the new instance
     </li>
    </ol>
    <pre><code>$ sudo rsync -var /data/galaxy-data-old/galaxy/database/files/* /data/galaxy-data/galaxy/database/files/
</code></pre>
    <ol start="9">
     <li>
      Copy all the installed tools
     </li>
    </ol>
    <pre><code>$ sudo rsync -var /data/galaxy-data-old/tool_deps/* /data/galaxy-data/tool_deps/
$ sudo rsync -var /data/galaxy-data-old/galaxy/database/shed_tools/* /data/galaxy-data/galaxy/database/shed_tools/
$ sudo rsync -var /data/galaxy-data-old/galaxy/database/config/* /data/galaxy-data/galaxy/database/config/
</code></pre>
    <ol start="10">
     <li>
      Copy the welcome page and all its files.
     </li>
    </ol>
    <pre><code>$ sudo rsync -var /data/galaxy-data-old/welcome* /data/galaxy-data/
</code></pre>
    <ol start="11">
     <li>
      Create an auxiliary docker in interactive mode and upgrade the database.
     </li>
    </ol>
    <pre><code>$ sudo docker run -it --rm -v /data/galaxy-data:/export quay.io/bgruening/galaxy /bin/bash
# Startup all processes
&gt; startup &amp;
#Upgrade the database to the most recent version
&gt; sh manage_db.sh upgrade
#Logout
&gt; exit
</code></pre>
    <ol start="12">
     <li>
      Start the docker and test
     </li>
    </ol>
    <pre><code>$ sudo docker start galaxy-instance
</code></pre>
    <ol start="13">
     <li>
      Clean the old container and image
     </li>
    </ol>
    <h3>
     Postgresql migration
     <a name="Postgresql-migration">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <p>
     In the 19.05 version, Postgresql was updated from version 9.3 to version 11.5. If you are upgrading from a version &lt;19.05, you will need to migrate the database.
You can do it the following way (based on the "The quick upgrade method" above):
    </p>
    <ol>
     <li>
      Stop Galaxy in the old container
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;old_container_name&gt; galaxyctl stop
</code></pre>
    <ol start="2">
     <li>
      Dump the old database
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;old_container_name&gt; bash
su postgres
pg_dumpall --clean &gt; /export/postgresql/9.3dump.sql
exit
exit
</code></pre>
    <ol start="3">
     <li>
      Update the container (= step 1 of the "The quick upgrade method" above)
     </li>
    </ol>
    <pre><code class="language-sh">docker stop &lt;old_container_name&gt;
docker pull quay.io/bgruening/galaxy
</code></pre>
    <ol start="4">
     <li>
      Run the container with the updated image (= step 2 of the "The quick upgrade method" above)
     </li>
    </ol>
    <pre><code class="language-sh">docker run -p 8080:80 -v /data/galaxy-data:/export --name &lt;new_container_name&gt; quay.io/bgruening/galaxy
</code></pre>
    <ol start="5">
     <li>
      Restore the dump to the new postgres version
     </li>
    </ol>
    <p>
     Wait for the startup process to finish (Galaxy should be accessible)
    </p>
    <pre><code class="language-sh">docker exec -it &lt;new_container_name&gt; bash
galaxyctl stop
su postgres
psql -f /export/postgresql/9.3dump.sql postgres
exit
exit
</code></pre>
    <ol start="6">
     <li>
      Use diff to find changes in the config files (only if you changed any config file). (= step 3 of the "The quick upgrade method" above)
     </li>
    </ol>
    <pre><code class="language-sh">cd /data/galaxy-data/.distribution_config
for f in *; do echo $f; diff $f ../galaxy/config/$f; read; done
</code></pre>
    <ol start="7">
     <li>
      Upgrade the database schema (= step 4 of the "The quick upgrade method" above)
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;new_container_name&gt; bash
galaxyctl stop
sh manage_db.sh upgrade
exit
</code></pre>
    <ol start="5">
     <li>
      Restart Galaxy (= step 5 of the "The quick upgrade method" above)
     </li>
    </ol>
    <pre><code class="language-sh">docker exec -it &lt;new_container_name&gt; galaxyctl start
</code></pre>
    <p>
     (Alternatively, restart the whole container)
    </p>
    <ol start="6">
     <li>
      Clean old files
     </li>
    </ol>
    <p>
     If you are
     <em>
      very
     </em>
     sure that everything went well, you can delete
     <code>
      /export/postgresql/9.3dump.sql
     </code>
     and
     <code>
      /export/postgresql/9.3/
     </code>
     to save some space.
    </p>
    <h2>
     Enabling Interactive Tools in Galaxy
     <a name="Enabling-Interactive-Tools-in-Galaxy">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     Interactive Tools (IT) are sophisticated ways to extend Galaxy with powerful services, like
     <a href="http://jupyter.org/">
      Jupyter
     </a>
     , in a secure and reproducible way.
    </p>
    <p>
     For this we need to be able to launch Docker containers inside our Galaxy Docker container.
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 8021:21 -p 4002:4002 \
    --privileged=true \
    -v /home/user/galaxy_storage/:/export/ \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     The port 4002 is the proxy port that is used to handle Interactive Tools.
     <code>
      --privileged
     </code>
     is needed to start docker containers inside docker.
    </p>
    <p>
     Additionally, you can set the
     <code>
      GALAXY_DOMAIN
     </code>
     environment variable to specify the domain name for your Galaxy instance to ensure that domain-based ITs work correctly. By default, it is set to
     <code>
      localhost
     </code>
     . If you have your own domain, you can specify it instead.
    </p>
    <p>
     If you're using the default job configuration, set the
     <code>
      GALAXY_DESTINATIONS_DEFAULT
     </code>
     environment variable to a Docker-enabled destination. By default, this is set to
     <code>
      slurm_cluster
     </code>
     , so you'll need to update it accordingly. Alternatively, you can also provide your own job configuration file.
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 8021:21 -p 4002:4002 \
    --privileged=true \
    -v /home/user/galaxy_storage/:/export/ \
    -e "GALAXY_DOMAIN=your.domain.com" \
    -e "GALAXY_DESTINATIONS_DEFAULT=slurm_cluster_docker" \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Using passive mode FTP or SFTP
     <a name="Using-passive-mode-FTP-or-SFTP">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     By default, FTP servers running inside of docker containers are not accessible via passive mode FTP, due to not being able to expose extra ports. To circumvent this, you can use the
     <code>
      --net=host
     </code>
     option to allow Docker to directly open ports on the host server:
    </p>
    <pre><code class="language-sh">docker run -d \
    --net=host \
    -v /home/user/galaxy_storage/:/export/ \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     Note that there is no need to specifically bind individual ports (e.g.,
     <code>
      -p 80:80
     </code>
     ) if you use
     <code>
      --net
     </code>
     .
    </p>
    <p>
     An alternative to FTP and it's shortcomings it to use the SFTP protocol via port 22. Start your Galaxy container with a port binding to 22.
    </p>
    <pre><code class="language-sh">docker run -i -t -p 8080:80 -p 8022:22 \
    -v /home/user/galaxy_storage/:/export/ \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     And use for example
     <a href="https://filezilla-project.org/">
      Filezilla
     </a>
     or the
     <code>
      sftp
     </code>
     program to transfer data:
    </p>
    <pre><code class="language-sh">sftp -v -P 8022 -o User=admin@example.org localhost &lt;&lt;&lt; $'put &lt;YOUR FILE HERE&gt;'
</code></pre>
    <h2>
     Using Parent docker
     <a name="Using-Parent-docker">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     On some linux distributions, Docker-In-Docker can run into issues (such as running out of loopback interfaces). If this is an issue, you can use a 'legacy' mode that use a docker socket for the parent docker installation mounted inside the container. To engage, set the environmental variable
     <code>
      DOCKER_PARENT
     </code>
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 -p 8021:21 \
    --privileged=true -e DOCKER_PARENT=True \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /home/user/galaxy_storage/:/export/ \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Galaxy Report Webapp
     <a name="Galaxy-Report-Webapp">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     For admins wishing to have more information on the status of a galaxy instance, the Galaxy Report Webapp is served on
     <code>
      http://localhost:8080/reports
     </code>
     . As default this site is password protected with
     <code>
      admin:admin
     </code>
     . You can change this by providing a
     <code>
      common_htpasswd
     </code>
     file in
     <code>
      /home/user/galaxy_storage/
     </code>
     .
    </p>
    <p>
     You can disable the Report Webapp entirely by providing the environment variable
     <code>
      NONUSE
     </code>
     during container startup.
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 \
    -e "NONUSE=reports" \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     RabbitMQ Management
     <a name="RabbitMQ-Management">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     RabbitMQ is used as the broker for services like Celery. RabbitMQ provides a dedicated web interface for managing message queues, accessible at
     <code>
      http://localhost:8080/rabbitmq/
     </code>
     . This interface allows you to monitor queues, exchanges, bindings, and more. By default, it is password protected with
     <code>
      admin:admin
     </code>
     , but the credentials can be changed after logging in.
    </p>
    <p>
     To completely disable RabbitMQ, you can set the
     <code>
      NONUSE
     </code>
     environment variable during container startup.
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 \
    -e "NONUSE=rabbitmq" \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Flower Webapp
     <a name="Flower-Webapp">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     Flower is a web-based tool for monitoring and administering Celery. It is accessible at
     <code>
      http://localhost:8080/flower
     </code>
     . By default, this site is password protected with
     <code>
      admin:admin
     </code>
     . You can change this by providing a
     <code>
      common_htpasswd
     </code>
     file in
     <code>
      /home/user/galaxy_storage/
     </code>
     .
    </p>
    <p>
     The Flower Webapp will only be available if both Celery and RabbitMQ are enabled, meaning the environment variable
     <code>
      NONUSE
     </code>
     does not include
     <code>
      celery
     </code>
     and
     <code>
      rabbitmq
     </code>
     . To completely disable the Flower Webapp, you can set the
     <code>
      NONUSE
     </code>
     environment variable during container startup.
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 \
    -e "NONUSE=flower" \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Galaxy's config settings
     <a name="Galaxys-config-settings">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     Every Galaxy configuration parameter in
     <code>
      config/galaxy.yml
     </code>
     can be overwritten by passing an environment variable to the
     <code>
      docker run
     </code>
     command during startup. The name of the environment variable has to be:
     <code>
      GALAXY_CONFIG
     </code>
     +
     <em>
      the_original_parameter_name_in_capital_letters
     </em>
     For example, you can set the Galaxy session timeout to 5 minutes and set your own Galaxy brand by invoking the
     <code>
      docker run
     </code>
     like this:
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 \
    -e "GALAXY_CONFIG_BRAND='My own Galaxy flavour'" \
    -e "GALAXY_CONFIG_SESSION_DURATION=5" \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     Note, that if you would like to run any of the
     <a href="https://galaxyproject.org/admin/config/performance/purge-histories-and-datasets/">
      cleanup scripts
     </a>
     , you will need to add the following to
     <code>
      /export/galaxy/config/galaxy.yml
     </code>
     :
    </p>
    <pre><code>database_connection = postgresql://galaxy:galaxy@localhost:5432/galaxy
file_path = /export/galaxy/database/files
</code></pre>
    <h2>
     Security Configuration
    </h2>
    <p>
     <em>
      By default
     </em>
     the
     <code>
      admin_users
     </code>
     and
     <code>
      bootstrap_admin_api_key
     </code>
     variables are set to:
    </p>
    <pre><code>admin_users: admin@example.org
bootstrap_admin_api_key: HSNiugRFvgT574F43jZ7N9F3
</code></pre>
    <p>
     Additionally Galaxy encodes various internal values that can be part of output using secret string configurable as
     <code>
      id_secret
     </code>
     in the config file (use 5-65 bytes long string). This prevents 'guessing' of Galaxy's internal database sequences. Example:
    </p>
    <pre><code>id_secret: d5c910cc6e32cad08599987ab64dcfae
</code></pre>
    <p>
     You should change all three configuration variables above manually in
     <code>
      /export/galaxy/config/galaxy.yml
     </code>
     .
    </p>
    <p>
     Alternatively you can pass the security configuration when running the image but please note that it is a security problem. E.g. if a tool exposes all
     <code>
      env
     </code>
     's your secret API key will also be exposed.
    </p>
    <h2>
     Configuring Galaxy's behind a proxy
     <a name="Galaxy-behind-proxy">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     If your Galaxy docker instance is running behind an HTTP proxy server, and if you're accessing it with a specific path prefix (e.g.
     <a href="http://www.example.org/some/prefix/">
      http://www.example.org/some/prefix/
     </a>
     ), you need to make Galaxy aware of it. There is an environment variable available to do so:
    </p>
    <pre><code>PROXY_PREFIX=/some/prefix
</code></pre>
    <p>
     You can and should overwrite these during launching your container:
    </p>
    <pre><code class="language-sh">docker run -p 8080:80 \
    -e "PROXY_PREFIX=/some/prefix" \
    quay.io/bgruening/galaxy
</code></pre>
    <h2>
     On-demand reference data with CVMFS
     <a name="cvmfs">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     By default, Galaxy instances launched with this image will have on-demand access to approximately 4TB of
reference genomes and indexes. These are the same reference data available on the main Galaxy server.
This is achieved by connecting to Galaxy's CernVM filesystem (CVMFS) at
     <code>
      cvmfs-config.galaxyproject.org
     </code>
     repository, which provides automatic configuration for all galaxyproject.org CVMFS repositories, including
     <code>
      data.galaxyproject.org
     </code>
     , and ensures they remain up to date.
The CVMFS capability doesn't add to the size of the Docker image, but when running, CVMFS maintains
a cache to keep the most recently used data on the local disk.
    </p>
    <p>
     <em>
      Note
     </em>
     : for CVMFS directories to be mounted-on-demand with
     <code>
      autofs
     </code>
     , you must launch Docker as
     <code>
      --privileged
     </code>
    </p>
    <h2>
     Personalize your Galaxy
     <a name="Personalize-your-Galaxy">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     The Galaxy welcome screen can be changed by providing a
     <code>
      welcome.html
     </code>
     page in
     <code>
      /home/user/galaxy_storage/
     </code>
     . All files starting with
     <code>
      welcome
     </code>
     will be copied during startup and served as introduction page. If you want to include images or other media, name them
     <code>
      welcome_*
     </code>
     and link them relative to your
     <code>
      welcome.html
     </code>
     (
     <a href="%60https://github.com/bgruening/docker-galaxy-stable/blob/master/galaxy/welcome.html%60">
      example
     </a>
     ).
    </p>
    <h2>
     Deactivating services
     <a name="Deactivating-services">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     Non-essential services can be deactivated during startup. Set the environment variable
     <code>
      NONUSE
     </code>
     to a comma separated list of services. Currently,
     <code>
      postgres
     </code>
     ,
     <code>
      cron
     </code>
     ,
     <code>
      proftp
     </code>
     ,
     <code>
      reports
     </code>
     ,
     <code>
      nodejs
     </code>
     ,
     <code>
      condor
     </code>
     ,
     <code>
      slurmd
     </code>
     ,
     <code>
      slurmctld
     </code>
     ,
     <code>
      celery
     </code>
     ,
     <code>
      rabbitmq
     </code>
     ,
     <code>
      redis
     </code>
     ,
     <code>
      flower
     </code>
     and
     <code>
      tusd
     </code>
     are supported.
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 9002:9002 \
    -e "NONUSE=cron,proftp,reports,nodejs,condor,slurmd,slurmctld,celery,rabbitmq,redis,flower,tusd" \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     A graphical user interface, to start and stop your services, is available on port
     <code>
      9002
     </code>
     if you run your container like above.
    </p>
    <h2>
     Restarting Galaxy
     <a name="Restarting-Galaxy">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     If you want to restart Galaxy without restarting the entire Galaxy container you can use
     <code>
      docker exec
     </code>
     (docker &gt; 1.3).
    </p>
    <pre><code class="language-sh">docker exec &lt;container name&gt; galaxyctl restart
</code></pre>
    <p>
     In addition, you can start/stop every supervisord process using a web interface on port
     <code>
      9002
     </code>
     . Start your container with:
    </p>
    <pre><code class="language-sh">docker run -p 9002:9002 quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Advanced Logging
     <a name="Advanced-Logging">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <p>
     You can set the environment variable $GALAXY_LOGGING to FULL to access all logs from supervisor. For example start your container with:
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 8021:21 \
    -e "GALAXY_LOGGING=full" \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     Then, you can access the supervisord web interface on port
     <code>
      9002
     </code>
     and get access to log files. To do so, start your container with:
    </p>
    <pre><code class="language-sh">docker run -d -p 8080:80 -p 8021:21 -p 9002:9002 \
    -e "GALAXY_LOGGING=full" \
    quay.io/bgruening/galaxy
</code></pre>
    <p>
     Alternatively, you can access the container directly using the following command:
    </p>
    <pre><code class="language-sh">docker exec -it &lt;container name&gt; bash
</code></pre>
    <p>
     Once connected to the container, log files are available in
     <code>
      /home/galaxy/logs
     </code>
     .
    </p>
    <p>
     A volume can also be used to map this directory to one external to the container - for instance if logs need to be persisted for auditing reasons (security, debugging, performance testing, etc...).:
    </p>
    <pre><code class="language-sh">mkdir gx_logs
docker run -d -p 8080:80 -p 8021:21 -e "GALAXY_LOGGING=full" -v `pwd`/gx_logs:/home/galaxy/logs quay.io/bgruening/galaxy
</code></pre>
    <h2>
     Running on an external cluster (DRM)
     <a name="Running-on-an-external-cluster-(DRM)">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h2>
    <h3>
     Basic setup for the filesystem
     <a name="Basic-setup-for-the-filesystem">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <h4>
     The easy way
    </h4>
    <p>
     The easiest way is to create a
     <code>
      /export
     </code>
     mount point on the cluster and mount the container with
     <code>
      /export:/export
     </code>
     .
    </p>
    <h4>
     Not using the /export mount point on the cluster.
    </h4>
    <p>
     The docker container sets up all its files on the /export directory, but this directory may not exist on the cluster filesystem. This can be solved with symbolic links on the cluster filesystem but it can also be solved within the container itself.
    </p>
    <p>
     In this example configuration the cluster file system has a directory
     <code>
      /cluster_storage/galaxy_data
     </code>
     which is accessible for the galaxy user in the container (UID 1450) and the user starting the container.
    </p>
    <p>
     The container should be started with the following settings configured:
    </p>
    <pre><code class="language-bash">docker run -d -p 8080:80 -p 8021:21
-v /cluster_storage/galaxy_data/galaxy_export:/export # This makes sure all galaxy files are on the cluster filesystem
-v /cluster_storage/galaxy_data:/cluster_storage/galaxy_data # This ensures the links within the docker container and on the cluster fs are the same
# The following settings make sure that each job is configured with the paths on the cluster fs instead of /export
-e GALAXY_CONFIG_TOOL_DEPENDENCY_DIR="/cluster_storage/galaxy_data/galaxy_export/tool_deps"
-e GALAXY_CONFIG_TOOL_DEPENDENCY_CACHE_DIR="/cluster_storage/galaxy_data/galaxy_export/tool_deps/_cache"
-e GALAXY_CONFIG_FILE_PATH="/cluster_storage/galaxy_data/galaxy_export/galaxy/database/files"
-e GALAXY_CONFIG_TOOL_PATH="/cluster_storage/galaxy_data/galaxy_export/galaxy/tools"
-e GALAXY_CONFIG_TOOL_DATA_PATH="/cluster_storage/galaxy_data/galaxy_export/galaxy/tool-data"
-e GALAXY_CONFIG_SHED_TOOL_DATA_PATH="/cluster_storage/galaxy_data/galaxy_export/galaxy/tool-data"
# The following settings are for directories that can be anywhere on the cluster fs.
-e GALAXY_CONFIG_JOB_WORKING_DIRECTORY="/cluster_storage/galaxy_data/galaxy_export/galaxy/database/job_working_directory" #IMPORTANT: needs to be created manually. Can also be placed elsewhere, but is originally located here
-e GALAXY_CONFIG_NEW_FILE_PATH="/cluster_storage/galaxy_data/tmp" # IMPORTANT: needs to be created manually. This needs to be writable by UID=1450 and have its flippy bit set (chmod 1777 for world-writable with flippy bit)
-e GALAXY_CONFIG_OUTPUTS_TO_WORKING_DIRECTORY=False # Writes Job scripts, stdout and stderr to job_working_directory.
-e GALAXY_CONFIG_RETRY_JOB_OUTPUT_COLLECTION=5 #IF your cluster fs uses nfs this may introduce latency. You can set galaxy to retry if a job output is not yet created.
# Conda settings. IMPORTANT!
-e GALAXY_CONFIG_CONDA_PREFIX="/cluster_storage/galaxy_data/_conda" # Can be anywhere EXCEPT cluster_storage/galaxy/galaxy_export!
# Conda uses $PWD to determine where the virtual environment is. If placed inside the export directory conda will determine $PWD to be a subirectory of the  /export folder which does not exist on the cluster!
-e GALAXY_CONFIG_CONDA_AUTO_INIT=True # When the necessary environment can not be found a new one will automatically be created
</code></pre>
    <h3>
     Setting up a Python virtual environment on the cluster
     <a name="Setting-up-a-python-virtual-environment-on-the-cluster">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <p>
     The Python environment in the container is not accessible from the cluster. So it needs to be created beforehand.
In this example configuration the Python virtual environment is created on
     <code>
      /cluster_storage/galaxy_data/galaxy_venv
     </code>
     and the export folder on
     <code>
      /cluster_storage/galaxy_data/galaxy_export
     </code>
     . To create the virtual environment:
    </p>
    <ol>
     <li>
      Create the virtual environment
      <code>
       virtualenv /cluster_storage/galaxy_data/galaxy_venv
      </code>
     </li>
     <li>
      Activate the virtual environment
      <code>
       source /cluster_storage/galaxy_data/galaxy_venv/bin/activate
      </code>
     </li>
     <li>
      Install the galaxy requirements
      <code>
       pip install --index-url https://wheels.galaxyproject.org/simple --only-binary all -r /cluster_storage/galaxy_data/galaxy/lib/galaxy/dependencies/pinned-requirements.txt
      </code>
     </li>
    </ol>
    <ul>
     <li>
      Make sure to upgrade the environment with the new requirements when a new version of galaxy is released.
     </li>
    </ul>
    <p>
     To make the Python environment usable on the cluster, create your custom
     <code>
      job_conf.xml
     </code>
     file and put it in
     <code>
      /cluster_storage/galaxy_data/galaxy_export/galaxy/config
     </code>
     .
In the destination section the following code should be added:
    </p>
    <pre><code class="language-xml">&lt;destinations default="cluster"&gt;
  &lt;destination id="cluster" runner="your_cluster_runner"&gt;
    &lt;env file="/cluster_storage/galaxy_data/galaxy_venv/bin/activate"/&gt;
    &lt;env id="GALAXY_ROOT_DIR"&gt;/cluster_storage/galaxy_data/galaxy_export/galaxy&lt;/env&gt;
    &lt;env id="GALAXY_LIB"&gt;/cluster_storage/galaxy_data/galaxy_export/galaxy/lib&lt;/env&gt;
    &lt;env id="PYTHONPATH"&gt;/cluster_storage/galaxy_data/galaxy_export/galaxy/lib&lt;/env&gt;
    &lt;param id="embed_metadata_in_job"&gt;True&lt;/param&gt;
  &lt;/destination&gt;
</code></pre>
    <p>
     In this way, Python tools on the cluster are able to use the Galaxy libraries.
    </p>
    <p>
     More information can be found
     <a href="https://github.com/galaxyproject/galaxy/blob/dev/doc/source/admin/framework_dependencies.rst#managing-dependencies-manually">
      here
     </a>
     and
     <a href="https://github.com/galaxyproject/galaxy/blob/dev/doc/source/admin/framework_dependencies.rst#galaxy-job-handlers">
      here
     </a>
     .
    </p>
    <h3>
     Using an external Slurm cluster
     <a name="Using-an-external-Slurm-cluster">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <p>
     It is often convenient to configure Galaxy to use a high-performance cluster for running jobs. To do so, two files are required:
    </p>
    <ol>
     <li>
      munge.key
     </li>
     <li>
      slurm.conf
     </li>
    </ol>
    <p>
     These files from the cluster must be copied to the
     <code>
      /export
     </code>
     mount point (i.e.,
     <code>
      /cluster_storage/galaxy_data/galaxy_export/
     </code>
     on the host if using below command) accessible to Galaxy before starting the container. This must be done regardless of which Slurm daemons are running within Docker. At start, symbolic links will be created to these files to
     <code>
      /etc
     </code>
     within the container, allowing the various Slurm functions to communicate properly with your cluster. In such cases, there's no reason to run
     <code>
      slurmctld
     </code>
     , the Slurm controller daemon, from within Docker, so specify
     <code>
      -e "NONUSE=slurmctld"
     </code>
     . Unless you would like to also use Slurm (rather than the local job runner) to run jobs within the Docker container, then alternatively specify
     <code>
      -e "NONUSE=slurmctld,slurmd"
     </code>
     .
    </p>
    <p>
     Importantly, Slurm relies on a shared filesystem between the Docker container and the execution nodes. To allow things to function correctly, checkout the basic filesystem setup above.
    </p>
    <p>
     A brief note is in order regarding the version of Slurm installed. This Docker image uses Ubuntu 14.04 as its base image. The version of Slurm in the Ubuntu 14.04 repository is 2.6.5 and that is what is installed in this image. If your cluster is using an incompatible version of Slurm then you will likely need to modify this Docker image.
    </p>
    <p>
     The following is an example for how to specify a destination in
     <code>
      job_conf.xml
     </code>
     that uses a custom partition ("work", rather than "debug") and 4 cores rather than 1:
    </p>
    <pre><code>&lt;destination id="slurm4threads" runner="slurm"&gt;
    &lt;param id="embed_metadata_in_job"&gt;False&lt;/param&gt;
    &lt;param id="nativeSpecification"&gt;-p work -n 4&lt;/param&gt;
&lt;/destination&gt;
</code></pre>
    <p>
     The usage of
     <code>
      -n
     </code>
     can be confusing. Note that it will specify the number of cores, not the number of tasks (i.e., it's not equivalent to
     <code>
      srun -n 4
     </code>
     ).
    </p>
    <h3>
     Using an external Grid Engine cluster
     <a name="Using-an-external-Grid-Engine-cluster">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <p>
     Set up the filesystem on the cluster as mentioned above.
To use Grid Engine (Sun Grid Engine, Open Grid Scheduler), one configuration file and an environment variable are required:
    </p>
    <ol>
     <li>
      create an
      <code>
       act_qmaster
      </code>
      file in the /export folder.
     </li>
    </ol>
    <ul>
     <li>
      In
      <em>
       <strong>
        act_qmaster
       </strong>
      </em>
      is something like this.
     </li>
    </ul>
    <pre><code>YOUR_GRIDENGINE_MASTER_HOST
</code></pre>
    <ul>
     <li>
      this file will automatically be installed in the container's
      <code>
       /var/lib/gridengine
      </code>
      folder.
     </li>
    </ul>
    <ol start="2">
     <li>
      set the environment variable
      <code>
       SGE_ROOT
      </code>
     </li>
    </ol>
    <ul>
     <li>
      By default
     </li>
    </ul>
    <pre><code>-e SGE_ROOT=/var/lib/gridengine
</code></pre>
    <ol start="3">
     <li>
      Make sure that YOUR_GRIDENGINE_MASTER_HOST can be pinged from the docker container. If this is not the case you can put the qmaster's hostname and ip in the containers
      <code>
       /etc/hosts
      </code>
      Your Grid Engine needs to accept job submissions from inside the container. If your container is already on a host that can submit jobs, set the hostname of the container to be exactly the same as the host. (The hostname can be changed by using the --hostname flag when starting the container).
     </li>
    </ol>
    <p>
     Alternatively, you can add the container's hostname (default=galaxy-docker) to the /etc/hosts file on the gridengine head node. And setting the container's hostname as a submit host.
    </p>
    <h3>
     Tips for Running Jobs Outside the Container
     <a name="Tips-for-Running-Jobs-Outside-the-Container">
     </a>
     <a href="#toc">
      [toc]
     </a>
    </h3>
    <p>
     In its default state Galaxy assumes both the Galaxy source code and
various temporary files are available on shared file systems across the
cluster. When using Condor or SLURM (as described above) to run jobs outside
of the Docker container one can take steps to mitigate these assumptions.
    </p>
    <p>
     The
     <code>
      embed_metadata_in_job
     </code>
     option on job destinations in
     <code>
      job_conf.xml
     </code>
     forces Galaxy collect metadata inside the container instead of on the
cluster:
    </p>
    <pre><code>&lt;param id="embed_metadata_in_job"&gt;False&lt;/param&gt;
</code></pre>
    <p>
     This has performance implications and may not scale as well as performing
these calculations on the remote cluster - but this should not be a problem
for most Galaxy instances.
    </p>
   </section>
   <footer>
    <p>
     This project is maintained by
     <a href="https://github.com/bgruening">
      bgruening
     </a>
    </p>
    <p>
     <small>
      Hosted on GitHub Pages — Theme by
      <a href="https://github.com/orderedlist">
       orderedlist
      </a>
     </small>
    </p>
   </footer>
   <script src="js/landing_page.js">
   </script>
  </div>
 </body>
</html>
